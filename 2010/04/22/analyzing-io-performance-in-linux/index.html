<!DOCTYPE html>
<html>

    <head>
        <title> Analyzing I/O performance in Linux &middot;  </title>

        <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.80.0" />


<script   src="https://code.jquery.com/jquery-3.1.1.min.js"   integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8="   crossorigin="anonymous"></script>


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">


<link rel="stylesheet" href="https://cmdln.org/css/nix.css">
<link rel="stylesheet" href="https://cmdln.org/css/syntax.css">


<link href="https://fonts.googleapis.com/css?family=Inconsolata|Open+Sans|Roboto|Montserrat|Concert+One" rel="stylesheet">




    </head>

    <body>
        <header>
<nav class="navbar navbar-default navbar-fixed-top navbar-inverse font-header">
	<div class="container-fluid">
		<div class="navbar-header">
			<button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1" aria-expanded="false">
				<span class="sr-only">Toggle navigation</span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
			</button>
			<a class="navbar-brand" id="green-terminal" href=https://cmdln.org>nick@cmdln.org ~ $</a>
		</div>

		
		<div class="collapse navbar-collapse" id="navbar-collapse-1">
			<ul class="nav navbar-nav navbar-right">
				<li>
					<a href="https://cmdln.org">/home/nick</a>
				</li>
				
				
				<li >
					<a href="/post/">~/posts</a>
				</li>
				

			</ul>
		</div>
	</div>
</nav>
</header>

        <div class="container wrapper">
            <h1><a href="https://cmdln.org/2010/04/22/analyzing-io-performance-in-linux/">Analyzing I/O performance in Linux</a></h1>
            <span class="post-date">Apr 22, 2010 </span>
            <div class="post-content">
                <p>Monitoring and analyzing performance is an important task for any sysadmin. Disk I/O bottlenecks can bring applications to a crawl. What are IOPS?  Should I use SATA, SAS, or FC? How many spindles do I need? What RAID level should I use? Is my system read or write heavy? These are common questions for anyone embarking on an disk I/O analysis quest. Obligatory disclaimer: I do not consider myself an expert in storage or anything for that mater. This is just how I have done I/O analysis in the past. I welcome additions and corrections. I believe it’s also important to note that this analysis is geared toward random operations than sequential read/write workloads.</p>
<p>Let’s start at the very beginning … a very good place to start. Hey it worked for Julie Andrews … So what are IOPS? They are input output (I/O) operations measured in seconds. It’s good to note that IOPS are also referred to as transfers per second (tps). IOPs are important for applications that require frequent access to disk. Databases, version control systems, and mail stores all come to mind.</p>
<p>Great so now that I know what IOPS are how do I calculate them? IOPS are a function of rotational speed (aka spindle speed), latency and seek time. The equation is pretty simple, 1/(seek + latency) = IOPS. Scott Lowe has a <a href="http://blogs.techrepublic.com.com/datacenter/?p=2182">good example</a> on his techreplublic.com blog.</p>
<blockquote>
<p>Sample drive:</p>
<!-- raw HTML omitted -->
<ul>
<li>Calculated IOPS for this disk: 1/(0.003 + 0.0045) = about 133 IOPS
It’s great to know how to calculate a disks IOPS but for the most part you can get by with commonly accepted averages. Of course sources vary but from what I have seen.</li>
</ul>
</blockquote>
<blockquote>
<!-- raw HTML omitted -->
<pre><code>&lt;td&gt;
  &lt;strong&gt;IOPS&lt;/strong&gt;
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;td&gt;
  50-80
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;td&gt;
  75-100
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;td&gt;
  125-150
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;td&gt;
  175-210
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
</blockquote>
<p>Should I use SATA, SAS or FC? That’s a loaded question. As with most things the answer is “depends”. I don’t want to get into the SATA vs SAS debate you can do your own research and make your own decisions based on your needs, but I will point out a few things.</p>
<ul>
<li>SATA only gets up to 10k (at the time of this writing)</li>
<li>SATA is only 1/2 duplex (From Tomak in comments)</li>
<li>Differences in reliability (<a href="http://en.wikipedia.org/wiki/Mean_time_between_failures">MTBF</a>, <a href="http://en.wikipedia.org/wiki/Bit_error_rate">BER</a>) interesting article on [Monitoring and analyzing performance is an important task for any sysadmin. Disk I/O bottlenecks can bring applications to a crawl. What are IOPS?  Should I use SATA, SAS, or FC? How many spindles do I need? What RAID level should I use? Is my system read or write heavy? These are common questions for anyone embarking on an disk I/O analysis quest. Obligatory disclaimer: I do not consider myself an expert in storage or anything for that mater. This is just how I have done I/O analysis in the past. I welcome additions and corrections. I believe it’s also important to note that this analysis is geared toward random operations than sequential read/write workloads.</li>
</ul>
<p>Let’s start at the very beginning … a very good place to start. Hey it worked for Julie Andrews … So what are IOPS? They are input output (I/O) operations measured in seconds. It’s good to note that IOPS are also referred to as transfers per second (tps). IOPs are important for applications that require frequent access to disk. Databases, version control systems, and mail stores all come to mind.</p>
<p>Great so now that I know what IOPS are how do I calculate them? IOPS are a function of rotational speed (aka spindle speed), latency and seek time. The equation is pretty simple, 1/(seek + latency) = IOPS. Scott Lowe has a <a href="http://blogs.techrepublic.com.com/datacenter/?p=2182">good example</a> on his techreplublic.com blog.</p>
<blockquote>
<p>Sample drive:</p>
<!-- raw HTML omitted -->
<ul>
<li>Calculated IOPS for this disk: 1/(0.003 + 0.0045) = about 133 IOPS
It’s great to know how to calculate a disks IOPS but for the most part you can get by with commonly accepted averages. Of course sources vary but from what I have seen.</li>
</ul>
</blockquote>
<blockquote>
<!-- raw HTML omitted -->
<pre><code>&lt;td&gt;
  &lt;strong&gt;IOPS&lt;/strong&gt;
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;td&gt;
  50-80
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;td&gt;
  75-100
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;td&gt;
  125-150
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;td&gt;
  175-210
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
</blockquote>
<p>Should I use SATA, SAS or FC? That’s a loaded question. As with most things the answer is “depends”. I don’t want to get into the SATA vs SAS debate you can do your own research and make your own decisions based on your needs, but I will point out a few things.</p>
<ul>
<li>SATA only gets up to 10k (at the time of this writing)</li>
<li>SATA is only 1/2 duplex (From Tomak in comments)</li>
<li>Differences in reliability (<a href="http://en.wikipedia.org/wiki/Mean_time_between_failures">MTBF</a>, <a href="http://en.wikipedia.org/wiki/Bit_error_rate">BER</a>) interesting article on]<a href="http://storageadvisors.adaptec.com/2005/11/02/actual-reliability-calculations-for-raid/">4</a></li>
<li>See differences in <a href="http://www.hardwaresecrets.com/article/315">Native Command Queuing (NCQ) and Command Tag Queuing (CTQ)</a></li>
</ul>
<p>These factors are  key considerations when choosing what kind of drives to use.</p>
<p>What RAID level should I use? You know what IOPS are, how to calculate them and determined what kind of drives to use, the next logical question is commonly RAID 5 vs RAID 10. There is difference in reliability, especially as the number of drives in your raid-set increases but that is outside the scope of this post.</p>
<!-- raw HTML omitted -->
<pre><code>&lt;td&gt;
  &lt;strong&gt;Write Operations&lt;/strong&gt;
&lt;/td&gt;

&lt;td&gt;
  &lt;strong&gt;Read Operations &lt;/strong&gt;
&lt;/td&gt;

&lt;td&gt;
  &lt;strong&gt;Notes &lt;/strong&gt;
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;td style=&quot;text-align: center;&quot;&gt;
  1
&lt;/td&gt;

&lt;td style=&quot;text-align: center;&quot;&gt;
  1
&lt;/td&gt;

&lt;td style=&quot;text-align: center;&quot;&gt;
  &lt;strong&gt;Write&lt;/strong&gt;/&lt;strong&gt;Read&lt;/strong&gt;: high throughput, low CPU utilization, no redundancy
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;td style=&quot;text-align: center;&quot;&gt;
  2
&lt;/td&gt;

&lt;td style=&quot;text-align: center;&quot;&gt;
  1
&lt;/td&gt;

&lt;td style=&quot;text-align: center;&quot;&gt;
  &lt;strong&gt;Write&lt;/strong&gt;: only as fast as single drive&lt;strong&gt;Read&lt;/strong&gt;: Two read schemes available. Read data from both drives, or data from the drive that returns it first. One is higher throughput the other is faster seek times.
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;td style=&quot;text-align: center;&quot;&gt;
  4
&lt;/td&gt;

&lt;td style=&quot;text-align: center;&quot;&gt;
  1
&lt;/td&gt;

&lt;td style=&quot;text-align: center;&quot;&gt;
  &lt;strong&gt;Write&lt;/strong&gt;: Read-Modify-Write requires two reads and two writes per write request. Lower throughput higher CPU if the HBA doesn’t have a dedicated IO processor.Read-Modify-Write requires two reads and two writes per write request. Lower throughput higher CPU if the HBA doesn’t have a dedicated IO processor.&lt;strong&gt;Read&lt;/strong&gt;: High throughput low CPU utilization normally, in a failed state performance falls dramatically due to parity calculation and any rebuild operations that are going on.
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;td style=&quot;text-align: center;&quot;&gt;
  5
&lt;/td&gt;

&lt;td style=&quot;text-align: center;&quot;&gt;
  1
&lt;/td&gt;

&lt;td style=&quot;text-align: center;&quot;&gt;
  &lt;strong&gt;Write&lt;/strong&gt;: Read-Modify-Write requires three reads and three writes per write request. Do not use a software implementation if it is available&lt;strong&gt;Read&lt;/strong&gt;: High throughput low CPU utilization normally, in a failed state performance falls dramatically due to parity calculation and any rebuild operations that are going on.
&lt;/td&gt;
</code></pre>
<!-- raw HTML omitted -->
<p>As you can see in the table above, writes are where you take your performance hit. Now that the penalty or RAID factor is known for different raid levels we can get a good estimate of the theoretical maximum IOPS for a RAID set (excluding caching of course). To do this you take the product of the number of disks and IOPS per disk divided by the sum of the %read workload and the product of the raid factor (see write operations column) and %write workload.</p>
<p>Here is the equation:</p>
<p>d = number of disks</p>
<p>dIOPS = IOPS per disk</p>
<p>%r = % of read workload</p>
<p>%w = % of write workload</p>
<p>F = raid factor (write operations column)</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<blockquote>
<p>Wait a second, where am I supposed to get %read and %write from?</p>
</blockquote>
<p>You need to examine your workload. I usually turn to my favorite statistics collector, sysstat.  sar -d -p will report activity for each block device and pretty print the device name. I am assuming you already know what block device you are looking to analyze but if your looking for the busiest device just look in the tps column.  the rd_sec/s and wr_sec/s columns display number of sectors read/written from/to the device. To get the percentage of read or writes divide rd_sec/s by the sum of rd_sec/s and wr_sec/s.</p>
<p>The equations:</p>
<!-- raw HTML omitted -->
<p><a href="http://www.cmdln.org/images/wp-content/uploads/2010/04/CodeCogsEqn6.gif"><!-- raw HTML omitted --></a></p>
<p>An example from my workstation:</p>
<p>Average for sdb rd_sec/s = 1150.80</p>
<p>Average for sdb wr_sec/s = 1166.53</p>
<!-- raw HTML omitted -->
<p><a href="http://www.cmdln.org/images/wp-content/uploads/2010/04/CodeCogsEqn14.gif"><!-- raw HTML omitted --></a></p>
<p>As you can see my workstation read/write workload is pretty balanced at 49.6% read, and 50.3% write. Compare that to a cvs server (don’t get me started on how bad cvs is, its just something I have to deal with).</p>
<p>Average for sdb rd_sec/s = 27.78k</p>
<p>Average for sdb wr_sec/s = 2.07k</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>This server workload is extremely high on reads. Ok time to analyze the performance.</p>
<p>In and of itself being a heavy read workload is not a problem. My problem is user complaints of slowness. I note (again from sysstat collected metrics) that the tps or average IOPS on this device is about 574. Again thats not an issue in and of itself, we need to know what we can expect from its subsystem. This device happens to be SAN based storage. The raid set its on is comprised of 4 10kRPM FC drives in a raid 10. Remember from the table above that IOPS for a 10kRPM drive are in the 125-150ish range. We need to calculate the expected IOPS from that raid set using the IOPS equation above, our measured workloads for read/write, the number of disks, and the raid level (10 and 1 are treated the same).</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Don’t get stuck where I am and have to guess how many spindles need to be added to reduce the pain, start recording your trends now! Even better, once you start collecting your statistical information go ahead and set an alert for 65% or 70% utilization of theoretical max IOPS for an extended period as well as increasingly bothersome alerts going up from there. It’s never good to have to react to performance issues, always better to be proactive. There was absolutely nothing wrong with the sizing of this example raid set 2-4 years ago. Had it been under monitoring the entire time with proper thresholds set a proper plan could have been made, and spindles could have been added before causing users any pain.</p>
<p>If you want to use sysstat like I did, you might find this Nagios plug-in that I wrote helpful <a href="http://github.com/nickanderson/check-sar-perf">check_sar_perf</a>. I use it with Zenoss, but it could be tied into any NMS that records the performance data from a Nagios plug-in.</p>
<p>Go forth, collect, analyze and plan so your users aren’t calling you with issues.</p>
<ul>
<li><a href="http://wiki.horde.org/HardwareRequirements">http://wiki.horde.org/HardwareRequirements</a></li>
<li><a href="http://don.blogs.smugmug.com/2007/10/08/hdd-iops-limiting-factor-seek-or-rpm/">http://don.blogs.smugmug.com/2007/10/08/hdd-iops-limiting-factor-seek-or-rpm/</a></li>
<li><a href="http://blogs.techrepublic.com.com/datacenter/?p=2182">http://blogs.techrepublic.com.com/datacenter/?p=2182</a></li>
<li><a href="http://www.sqlservercentral.com/blogs/sqlmanofmystery/archive/2009/12/07/fundamentals-of-storage-systems-raid-an-introduction.aspx">http://www.sqlservercentral.com/blogs/sqlmanofmystery/archive/2009/12/07/fundamentals-of-storage-systems-raid-an-introduction.aspx</a></li>
<li><a href="http://blog.aarondelp.com/2009/10/its-now-all-about-iops.html">http://blog.aarondelp.com/2009/10/its-now-all-about-iops.html</a></li>
<li><a href="http://adamstechblog.com/2009/02/10/how-to-calculate-iops-ios-per-second/">http://adamstechblog.com/2009/02/10/how-to-calculate-iops-ios-per-second/</a></li>
<li><a href="http://www.performancewiki.com/diskio-tuning.html">http://www.performancewiki.com/diskio-tuning.html</a></li>
<li><a href="http://vmtoday.com/2009/12/storage-basics-part-i-intro/">http://vmtoday.com/2009/12/storage-basics-part-i-intro/</a></li>
<li><a href="http://vmtoday.com/2009/12/storage-basics-part-ii-iops/">http://vmtoday.com/2009/12/storage-basics-part-ii-iops/</a></li>
<li><a href="http://vmtoday.com/2010/01/storage-basics-part-iii-raid/">http://vmtoday.com/2010/01/storage-basics-part-iii-raid/</a></li>
<li><a href="http://vmtoday.com/2010/01/storage-basics-part-iv-interface/">http://vmtoday.com/2010/01/storage-basics-part-iv-interface/</a></li>
<li><a href="http://vmtoday.com/2010/01/storage-basics-part-iv-interface/">http://vmtoday.com/2010/03/storage-basics-part-v-controllers-cache-and-coalescing/</a></li>
<li><a href="http://vmtoday.com/2010/04/storage-basics-part-vi-storage-workload-characterization/">http://vmtoday.com/2010/04/storage-basics-part-vi-storage-workload-characterization/</a></li>
<li><a href="http://www.codecogs.com/components/equationeditor/equationeditor.php">http://www.codecogs.com/components/equationeditor/equationeditor.php</a></li>
</ul>

            </div>
            
            <div class="post-comments">
                
            </div>
            
            <div class="push"></div>
        </div>
        <footer class="footer text-center">
<p>Copyright &copy; 2022 Nick Anderson -
<span class="credit">
	Powered by
	<a target="_blank" href="https://gohugo.io">Hugo</a>
	and
	<a target="_blank" href="https://github.com/LordMathis/hugo-theme-nix/">Nix</a> theme.
</span>
</p>
</footer>

    </body>
